{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETS = '/assets'\n",
    "DATASET_FOLDER = ASSETS + '/datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split medical records proportionally in each federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Set the dataset names generated by `src/create_dataset.py`\n",
    "datasets = {\n",
    "  \"medmcqa\": {\n",
    "    \"dataset\": load_dataset(join(DATASET_FOLDER,\"2024_09_10_10_30_51-medmcqa_None-val_size_0.1-max_input_length_1024\"), num_proc=64),\n",
    "  },\n",
    "  \"pubmedqa\": {\n",
    "    \"dataset\": load_dataset(join(DATASET_FOLDER,\"2024_10_22_10_24_48-pubmedqa_1k_50000-val_size_0.1-max_input_length_1024\"), num_proc=64),\n",
    "  },\n",
    "  \"flashcard\": {\n",
    "    \"dataset\": load_dataset(join(DATASET_FOLDER,\"2024_11_12_11_01_18-flashcard_None-val_size_0.1-max_input_length_1024\"), num_proc=64),\n",
    "  },\n",
    "  \"PHI\": {\n",
    "    \"dataset\": load_dataset(join(DATASET_FOLDER,\"2024_10_22_10_31_43-PHI_None-val_size_0.1-max_input_length_1024\"), num_proc=64),\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "  dataset['size'] = dataset['dataset']['train'].size_in_bytes + dataset['dataset']['validation'].size_in_bytes\n",
    "  print(name, dataset['size'] / 1024 // 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = datasets['medmcqa']['size'] + datasets['pubmedqa']['size'] + datasets['flashcard']['size']\n",
    "for name, dataset in datasets.items():\n",
    "  if name == 'PHI':\n",
    "      continue\n",
    "  dataset['PHI_ratio'] = dataset['size'] / total\n",
    "  print(dataset['PHI_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI_train = datasets['PHI']['dataset']['train'].shuffle(42)\n",
    "PHI_val = datasets['PHI']['dataset']['validation'].shuffle(42)\n",
    "PHI_train.num_rows, PHI_val.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in datasets.items():\n",
    "  if name == 'PHI':\n",
    "    continue\n",
    "  dataset['PHI_train_rows'] = int(PHI_train.num_rows * dataset['PHI_ratio'])\n",
    "  dataset['PHI_val_rows'] = int(PHI_val.num_rows * dataset['PHI_ratio'])\n",
    "  print(dataset['PHI_train_rows'], dataset['PHI_val_rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits, val_splits = [], []\n",
    "\n",
    "train_splits.append(datasets['medmcqa']['PHI_train_rows'])\n",
    "val_splits.append(datasets['medmcqa']['PHI_val_rows'])\n",
    "\n",
    "train_splits.append(train_splits[0] + datasets['pubmedqa']['PHI_train_rows'])\n",
    "val_splits.append(val_splits[0] + datasets['pubmedqa']['PHI_val_rows'])\n",
    "train_splits, val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_splits_idx = np.split(np.arange(PHI_train.num_rows), train_splits)\n",
    "val_splits_idx = np.split(np.arange(PHI_val.num_rows), val_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "  if name == 'PHI':\n",
    "    continue\n",
    "  \n",
    "  datasets[name]['new_dataset'] = DatasetDict({\n",
    "    'train': concatenate_datasets([dataset['dataset']['train'], PHI_train.select(train_splits_idx[i])]),\n",
    "    'validation': concatenate_datasets([dataset['dataset']['validation'], PHI_val.select(val_splits_idx[i])]),\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, d in datasets.items():\n",
    "  if name == 'PHI':\n",
    "    continue\n",
    "  dataset = d['new_dataset']\n",
    "  dataset.set_format(\"torch\", device=\"cuda\")\n",
    "  dataset_filename = f\"{name}\"\n",
    "\n",
    "  output_path = join(DATASET_FOLDER, \"federated/2024_11_12_13_06_32_PHI_proportional_splits\", dataset_filename)\n",
    "  dataset.save_to_disk(output_path, num_proc=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
