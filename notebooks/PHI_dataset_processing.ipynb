{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this notebook is to handle inconsistent white space in the medical records.\n",
    "Some records are formatted with a newline character after each line reaches a certain length while\n",
    "others only uses newlines for paragraphs. We want to process the first case into the second one in order to prevent long succession of empty spaces in the data.\n",
    "\n",
    "The output of this notebook is a JSON file `PHI_dataset.json` containing preprocessed medical records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions used where pandas==2.2.1 and xmltodict==0.13.0\n",
    "%pip install pandas xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import re\n",
    "import json\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the path to point to the i2b2 dataset containing the xml files\n",
    "PHI_folder = 'path_to/2014_PHI_gold_sets/'\n",
    "PHI_dataset = PHI_folder + 'all'\n",
    "files = [file for file in listdir(PHI_dataset) if file.endswith('.xml')]\n",
    "print('Number of records:', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = {f.split('-')[0] for f in files}\n",
    "print('Number of patients:',len(patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_reports = pd.DataFrame([f.split('-') for f in files], columns=['patient_id', 'report_id'])\n",
    "patient_reports['report_id'] = patient_reports.report_id.map(lambda r: r[:2])\n",
    "print(\"Distribution of number of reports per patient:\")\n",
    "patient_reports.groupby('patient_id').count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "\n",
    "def extractXML(directory, filename):\n",
    "    \"\"\"Parses an xml file to get the medical record content and metadata\"\"\"\n",
    "    tree = ET.parse(directory + '/'+ filename)\n",
    "    root = tree.getroot()\n",
    "    xmlstr = ET.tostring(root, encoding='utf8', method='xml')\n",
    "\n",
    "    xml_dict = xmltodict.parse(xmlstr,dict_constructor=dict)[\"deIdi2b2\"]\n",
    "    text = xml_dict[\"TEXT\"]\n",
    "    tags_dict = xml_dict[\"TAGS\"]\n",
    "\n",
    "    return text,tags_dict,xmlstr\n",
    "\n",
    "def is_line_to_remove(prev_line, line):\n",
    "    \"\"\"\n",
    "    Remove empty lines or only containing separators\n",
    "    xxx-yy: lines of dashes -\n",
    "    \"\"\"\n",
    "    is_undesirable_line = line == ''\n",
    "    is_undesirable_line |= len(set(line + '_ ')) == 2 # only chars are underscores and spaces e.g. ___ ______\n",
    "    is_undesirable_line |= len(set(line + '- ')) == 2 # only chars are dashes and spaces e.g. -- --\n",
    "    is_undesirable_line |= '******' in line\n",
    "    is_undesirable_line |= prev_line == '\\n'\n",
    "    return is_undesirable_line\n",
    "\n",
    "def is_list_item(prev_line, curr_line):\n",
    "    \"\"\"\n",
    "    We want to remove arbitrary newline characters within the text. To do so we look at the length of each line\n",
    "    and try to guess whether the newline charater is arbitrary or justified. If the line is part of a list then\n",
    "    we leave the newline as it is.\n",
    "    Patient records with interesting list patterns:\n",
    "    xxx-yy: (1) formats with tab and spaces\n",
    "    xxx-yy: only instance of #1: format\n",
    "    xxx-yy: very short sentence formatting, which can be messed up by testing the line length\n",
    "    xxx-yy: 1) a) i) formats\n",
    "    xxx-yy: 1. a. formats interlaced and tables\n",
    "    xxx-yy: 1.) format, items with ':', tables\n",
    "    xxx-yy: only instance of #1 #2 format currently matched by the length < 50 but not the regexes\n",
    "    \"\"\"\n",
    "    # the previous line has few characters, which is probably a list item, a date, name etc\n",
    "    if len(prev_line) < 50:\n",
    "        return True\n",
    "    if curr_line.startswith('-') or curr_line.startswith('#'):\n",
    "        return True\n",
    "    if ':' in prev_line and ':' in curr_line:\n",
    "        return True\n",
    "\n",
    "    # Identify list format like 1) or (1) or 1. or 1.) \n",
    "    # as well as a) b) or a. b. and roman numerals i) ii) or (ii) or iii. or iii.\n",
    "    # numbered lists go up to 15) and lettered lists go up to i)\n",
    "    if len(re.findall(r'^\\(?(\\d+|[a-i])(\\.|\\)|:)',curr_line.split(' ')[0])) > 0:\n",
    "        return True\n",
    "    # roman numerals i) ii) or (ii) or iii. or iii.\n",
    "    # roman numeral lists go up to iii in this dataset (no iv or v)\n",
    "    if len(re.findall(r'^\\(?i+(\\.|\\))',curr_line.split(' ')[0])) > 0:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "mapping_table = str.maketrans({\n",
    "    '’': \"'\",\n",
    "    '·': '-',\n",
    "    '–': ':',\n",
    "    '“': '\"',\n",
    "    '”': '\"',\n",
    "    '‘': \"'\",\n",
    "    '½': '1/2'\n",
    "})\n",
    "\n",
    "def clean_line(line):\n",
    "    line = re.sub('\\s{2,}', ' ', line) # remove consecutive white space within the line\n",
    "    line.replace('&#8211;', '-') # encoding\n",
    "    # remove dashes if there are too many consecutive dashes in the str\n",
    "    # e.g. -----  ---- or ----- \n",
    "    if '----' in line: \n",
    "        line = line.replace('-','')\n",
    "    if '_____' in line:\n",
    "        line = line.replace('_','')\n",
    "    # Unescape html symbol entities like &#8220;\n",
    "    if '&#' in line: \n",
    "        line = html.unescape(line)\n",
    "        line = line.translate(mapping_table)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_dataset():\n",
    "    \"\"\"\n",
    "    Converts and process each xml file into a .txt file in a `clean` folder.\n",
    "    Returns a list of each record represented as a dict.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for filename in listdir(PHI_dataset):\n",
    "        if filename.endswith('.xml'):\n",
    "            patient_record = filename.split('.')[0]\n",
    "            try:\n",
    "                text, tags_dict, _ = extractXML(PHI_dataset, filename)\n",
    "                # prepend the record id, e.g. xxx-yy at the very beginning to be used as identifier when injected in the training set\n",
    "                lines = [f'{patient_record}\\n']\n",
    "                # Here we drop lines we don't want, remove empty space and inline text arbitrarily broken into multiple lines into a single line\n",
    "                for line in (l.strip() for l in text.split('\\n')):\n",
    "                    if not is_line_to_remove(lines[-1], line):\n",
    "                        line = clean_line(line)\n",
    "                        if line == '':\n",
    "                            break\n",
    "                        # If it is the first line\n",
    "                        if len(lines) == 1:\n",
    "                            lines.append(line)\n",
    "                        else:\n",
    "                            prev_line = lines[-1]\n",
    "                            # Prefix with newline when the previous line ends with a dot or if it is part of a list\n",
    "                            if prev_line[-1] == '.' or is_list_item(prev_line, line):\n",
    "                                line = '\\n'+ line\n",
    "                            # O.w. assume it is the continuation of the same sentence and \n",
    "                            # prefix with a white space to inline sentences that are originally formatted on multiple lines\n",
    "                            else:\n",
    "                                line = ' ' + line    \n",
    "                            lines.append(line)\n",
    "                clean_text = \"\".join(lines)\n",
    "                dataset.append({'id':patient_record, 'text': clean_text, 'tags': tags_dict})\n",
    "\n",
    "                with open(PHI_folder + 'clean/' + patient_record + '.txt', 'w') as f:\n",
    "                    f.write(clean_text)\n",
    "          \n",
    "            except Exception as e: \n",
    "                print(filename, e)\n",
    "    return sorted(dataset, key=lambda x: x['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tags(dataset):\n",
    "    \"\"\"Extracts PHI tags. We didn't make use of tags in our current work.\"\"\"\n",
    "    for record in dataset:\n",
    "        inlined_record_text = \"\".join(record['text'].split('\\n')) # remove newlines because tags don't include them\n",
    "        try:\n",
    "          if type(record['tags']) != list:\n",
    "            inlined_tag_list = []\n",
    "            for tag_type, tag_list in record['tags'].items():\n",
    "                if type(tag_list) is not list:\n",
    "                    tag_list = [tag_list]\n",
    "                for tag in tag_list:\n",
    "                    new_tag = {\n",
    "                        'type': tag_type,\n",
    "                        'subtype': tag['@TYPE'],\n",
    "                        'id': tag['@id'],\n",
    "                        'value': clean_line(tag['@text'].strip()),\n",
    "                    }\n",
    "\n",
    "                    # Escape the pattern because some tags include parentheses like phone numbers\n",
    "                    tag_start_idx = [[m.start(), m.end()] for m in re.finditer(re.escape(new_tag['value']), inlined_record_text)]\n",
    "                    # Every tag should occur in the text\n",
    "                    if len(tag_start_idx) == 0:\n",
    "                        print(record['id'], new_tag)\n",
    "                        raise Exception\n",
    "                    new_tag['start_indices'] = tag_start_idx\n",
    "                    inlined_tag_list.append(new_tag)\n",
    "            record['tags'] = sorted(inlined_tag_list, key=lambda x: x['id'])\n",
    "        except Exception as e:\n",
    "            print(record['id'], e)\n",
    "\n",
    "dataset = preprocess_raw_dataset()\n",
    "preprocess_tags(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our work, we experiment with duplicated notes to control for the effect of data duplication on memorization. Here we create a dataset without any data duplication. Data is duplicated for fine-tuning via the `create_dataset.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PHI_folder + 'PHI_dataset.json', 'w') as f:\n",
    "    f.write(json.dumps(dataset, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
