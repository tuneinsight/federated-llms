python $(dirname $0)/../src/sft.py \
  --model_name 'meta-llama/Llama-2-7b-hf' \
  --output 'llama-2-7B-lora-lr_1e-4-PHI_duplicated-flashcards-medmcqa-pubmedqa' \
  --dataset_name 'PHI_None-flashcard_10000-medmcqa_None-pubmedqa_1k_50000-val_size_0.1-max_input_length_1024' \
  --use_wandb True \
  --learning_rate 0.0001 \
  --max_input_length 1024 \
  --num_epochs 3 \
  --max_steps -1 \
  --eval_steps 150 \
  --save_steps 150 \
  --gradient_checkpointing False \
  --global_batch_size 64 \
  --per_device_batch_size 32 \
  --flash_attention True \
  --train_in_8bit False \
  --optim 'paged_adamw_32bit' \
  --use_lora True \
  --lora_r 16 \
  --lora_alpha 8 \
  --lora_dropout 0.05 \
  --lora_target_modules "[\"q_proj\",\"k_proj\",\"v_proj\",\"up_proj\",\"o_proj\",\"down_proj\",\"gate_proj\"]"